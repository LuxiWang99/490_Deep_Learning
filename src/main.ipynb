{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, LSTM\n",
    "import pickle \n",
    "import os\n",
    "import librosa\n",
    "from spectogram_dataloader import SpectogramDataLoader\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SR=44100\n",
    "# sentence_length = 60\n",
    "# BATCH_SIZE=15\n",
    "# WORD_SIZE = 4 * SR\n",
    "# EPOCH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wav2sentences(nums, label):\n",
    "#   length = sentence_length * SR\n",
    "#   final = [nums[i * length:(i + 1) * length] for i in range((len(nums) + length - 1) // length )][:-1]\n",
    "#   if len(final) >= 1\n",
    "#     labels = [label for i in range(len(final))]\n",
    "#     final = np.array(final)\n",
    "#     final = final.reshape(final.shape[0], -1, WORD_SIZE)\n",
    "#     return final, labels\n",
    "#   return [], []\n",
    "\n",
    "# # target_path is the csv/pickle file that strores the trackID -> label\n",
    "\n",
    "# def process_data(src, target_path):\n",
    "#   X = []\n",
    "#   y = []\n",
    "#   with open(target_path, 'rb') as handle:\n",
    "#     id2label = pickle.load(handle)\n",
    "#   for root, dirs, files in os.walk(src):\n",
    "#     for file in files:\n",
    "#       x, sr = librosa.load(os.path.join(root, file), sr=SR)\n",
    "#       filename, file_ext = os.path.splitext(file)\n",
    "#       X_temp, y_temp = wav2sentences(x, id2label[int(filename)])\n",
    "#       for x in X_temp:\n",
    "#         X.append(x)\n",
    "#       for label in y_temp:\n",
    "#         y.append(label)\n",
    "#   d = {\"data\": X, \"label\": y}\n",
    "#   with open(\"data.pickle\", 'wb') as handle:\n",
    "#     pickle.dump(d, handle)\n",
    "# process_data(\"./musicnet/wav\", \"./musicnet/musicnet_labels.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./data.pickle\", 'rb') as handle:\n",
    "#   d = pickle.load(handle)\n",
    "\n",
    "# X = d[\"data\"]\n",
    "# y = d[\"label\"]\n",
    "\n",
    "# num_label = len(set(y))\n",
    "# labels = list(set(y))\n",
    "# labels2ind = {}\n",
    "\n",
    "# for index in range(len(labels)):\n",
    "#   labels2ind.update({labels[index]: index}) \n",
    "\n",
    "# y = [labels2ind[a] for a in y]\n",
    "\n",
    "# X = np.array(X)\n",
    "# y = np.array(y)\n",
    "\n",
    "# Suppose we have a working X, and y, both 2D\n",
    "\n",
    "# X = X.reshape((-1, sentence_length // 4, WORD_SIZE))\n",
    "# # y = y.reshape((-1, )\n",
    "\n",
    "# X_train = X[:int(X.shape[0] * 0.8)]\n",
    "# X_test = X[int(X.shape[0] * 0.8):]\n",
    "\n",
    "# y_train = y[:int(X.shape[0] * 0.8)]\n",
    "# y_test = y[int(X.shape[0] * 0.8):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH=3 * 44100\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(1025, input_shape=(1025, 259), return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4, activation='sigmoid'))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train(model, train_generator, EPOCHS=20):\n",
    "    model.fit_generator(generator=train_generator, epochs=EPOCHS)\n",
    "\n",
    "def test(model, X_test, y_test):\n",
    "    num_corr = 0\n",
    "    for id, source in X_test.items():\n",
    "        splitted = [X_test[id][i * LENGTH : (i + 1) * LENGTH] for i in range((len(X_test[id]) + LENGTH) // LENGTH)][:-1]\n",
    "        label = y_test[id]\n",
    "        processed = []\n",
    "        for sentence in range(len(splitted)):\n",
    "            processed.append(np.log((np.abs(librosa.stft(splitted[sentence], hop_length=512, win_length=2048))**2) + sys.float_info.epsilon))\n",
    "        prediction = model.predict(np.array(processed))\n",
    "        counts = np.sum(prediction, 0)\n",
    "        predict_label = np.argmax(counts)\n",
    "        if predict_label == label:\n",
    "            num_corr += 1\n",
    "            print(\"right! prediction: \" + str(predict_label) + \", label: \" + str(label))\n",
    "        else:\n",
    "            print(\"wrong! prediction: \" + str(predict_label) + \", label: \" + str(label)\")\n",
    "    return float(num_corr) / len(X_test)\n",
    "\n",
    "def get_test_data(test_dir, labels_dict):\n",
    "    X_test = {}\n",
    "    y_test = {}\n",
    "    mappings = [\"Bar\", \"Cla\", \"Rom\", \"Mod\"]\n",
    "\n",
    "    for root, dirs, files in os.walk(test_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".npy\"):\n",
    "                x_ = np.load(os.path.join(root, file))\n",
    "                x_id = os.path.splitext(file)[0]\n",
    "                X_test[x_id] = x_\n",
    "                y_test[x_id] = mappings.index(labels_dict[int(x_id)])\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/estberg/Projects/490F/490_Deep_Learning/data/'\n",
    "train_song_ids_file = '/Users/estberg/Projects/490F/490_Deep_Learning/data/train/song_ids.pkl'\n",
    "test_song_ids_file = '/Users/estberg/Projects/490F/490_Deep_Learning/data/test/song_ids.pkl'\n",
    "labels_file = '/Users/estberg/Projects/490F/490_Deep_Learning/data/labels.pkl'\n",
    "\n",
    "with open(train_song_ids_file, 'rb') as handle:\n",
    "    train_ids = pickle.load(handle)\n",
    "\n",
    "with open(test_song_ids_file, 'rb') as handle:\n",
    "    test_ids = pickle.load(handle)\n",
    "\n",
    "with open(labels_file, 'rb') as handle:\n",
    "    labels = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1025, 259)\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/estberg/miniconda3/envs/490-final-project/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-14-5cba7dc8a8ed>\", line 4, in <module>\n",
      "    X_test, y_test = get_test_data(test_dir='/Users/estberg/Projects/490F/490_Deep_Learning/data/test/', labels_dict=labels)\n",
      "  File \"<ipython-input-13-f15903cf7b98>\", line 36, in get_test_data\n",
      "    x_ = np.load(os.path.join(root, file))\n",
      "  File \"/Users/estberg/.local/lib/python3.6/site-packages/numpy/lib/npyio.py\", line 453, in load\n",
      "    pickle_kwargs=pickle_kwargs)\n",
      "  File \"/Users/estberg/.local/lib/python3.6/site-packages/numpy/lib/format.py\", line 738, in read_array\n",
      "    array = numpy.fromfile(fp, dtype=dtype, count=count)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/estberg/miniconda3/envs/490-final-project/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/estberg/miniconda3/envs/490-final-project/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/Users/estberg/miniconda3/envs/490-final-project/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/estberg/miniconda3/envs/490-final-project/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/Users/estberg/miniconda3/envs/490-final-project/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/Users/estberg/miniconda3/envs/490-final-project/lib/python3.6/inspect.py\", line 1448, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/Users/estberg/miniconda3/envs/490-final-project/lib/python3.6/inspect.py\", line 691, in getsourcefile\n",
      "    importlib.machinery.EXTENSION_SUFFIXES):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_generator = SpectogramDataLoader(train_ids, labels, \"/Users/estberg/Projects/490F/490_Deep_Learning/data/train/\", \n",
    "# batch_size=64)\n",
    "# print(train_generator.get_data_dim())\n",
    "# X_test, y_test = get_test_data(test_dir='/Users/estberg/Projects/490F/490_Deep_Learning/data/test/', labels_dict=labels)\n",
    "# model = get_model()\n",
    "# train(model, train_generator)\n",
    "# acc = test(model, X_test, y_test)\n",
    "\n",
    "# print(\"Accuracy: %.2f%%\" % (acc*100))\n",
    "\n",
    "# scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "# model.save_weights(\"model.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong!\n",
      "wrong!\n",
      "right!\n",
      "right!\n",
      "wrong!\n",
      "right!\n",
      "wrong!\n",
      "wrong!\n",
      "wrong!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "wrong!\n",
      "right!\n",
      "right!\n",
      "wrong!\n",
      "wrong!\n",
      "right!\n",
      "wrong!\n",
      "wrong!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "wrong!\n",
      "wrong!\n",
      "right!\n",
      "right!\n",
      "wrong!\n",
      "right!\n",
      "wrong!\n",
      "wrong!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "wrong!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "wrong!\n",
      "right!\n",
      "wrong!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "wrong!\n",
      "wrong!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "wrong!\n",
      "wrong!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "wrong!\n",
      "right!\n",
      "right!\n",
      "right!\n",
      "wrong!\n",
      "right!\n",
      "wrong!\n",
      "wrong!\n",
      "wrong!\n",
      "wrong!\n",
      "right!\n",
      "Accuracy: 61.33%\n"
     ]
    }
   ],
   "source": [
    "acc = test(model, X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
